{"name":"542 blog","tagline":"Musings about statistics and data science","body":"# A whole new world of errors\r\n\r\nHoekstra et al. have [written](http://www.ejwagenmakers.com/inpress/HoekstraEtAlPBR.pdf) about how university students do not have a precise understanding of confidence intervals (CIs), even when they have taken research methods courses. It turns out that that both trained and untrained minds look at the such classic intervals and want to believe—wrongly—that there is a 95% probability that the true parameter is in the interval [a,b]. \r\n\r\nAs an undergrad student of Political Science, I fell into this trap. But even after teaching myself to interpret confidence intervals correctly, it was not until I started working with complex surveys regularly that I realized how statistical software spit out confidence intervals that can often mislead researchers.\r\n\r\n<ins>The mistake is believing that these intervals capture all of your error.</ins> In fact, they only capture sampling error under idealized conditions. Sadly, reality is far from ideal, and non-sampling errors abound.\r\n\r\nThree types of non-sampling errors are particularly prevalent in the social sciences: non-response, under-coverage, and misspecification error. Let's jump on a magic carpet ride to explore these in turn.\r\n\r\n### Coming up short of your sampling target\r\n\r\nLet me illustrate **non-response error** with an [enterprise survey](https://jordanlens.org/research/) I led while working with the USAID Jordan Local Enterprise Support Project. In this study, we conducted a survey with businesses that would inform ideas for new economic interventions. The survey needed to be large enough that we could drill-down into marginal groups and discover new insights.\r\n\r\n<div style=\"text-align:center\">\r\n<img src=\"img/sample_losses.gif\" alt=\"Sample losses\">\r\n</div>\r\n\r\nAlthough we planned for a sample of 6,385 businesses, by the time we wrapped our fieldwork, we only managed to create a respondent set of 4,721 interviews. This is because some owners refused to take part, while other business could not be located or were shut at the time of visit. We believe these problems were prevalent for non-registered businesses who feared being reported to authorities, and Bedouin tradespeople in the South. \r\n\r\nAs far as [participation rates go](https://www.pewresearch.org/methods/2017/05/15/what-low-response-rates-mean-for-telephone-surveys/) for surveys, 74% is very desirable. Still, analyzing the USAID data without adjustment could lead to results that severely misstate the error. Why? Because statistical software assumes that data are “independent and identically distributed”—*i.i.d.* for short—but the non-response patterns mean that this condition no longer holds. Even for large datasets such as the USAID survey, nonresponse can be material enough to substantially understate the real width of your CIs.\r\n\r\n### The missing Uber driver\r\n\r\nWith the USAID survey, we wanted to build a master list of businesses that we could subsequently sample for an interview. Because we needed this list to capture the informal economy and gig-economy, our team had to build this sampling frame from scratch. We did this by going door-to-door in residential and commercial neighborhoods (sampled at random), and asking if a business existed at those addresses. \r\n\r\nWhile this mechanism was pretty good, it undoubtedly missed certain businesses — businesses like Uber drivers, who might be out on the roads at the time we visited, or gig-workers with day jobs in larger companies. Samples drawn from this imperfect frame would not include those self-employed businesses and would therefore be biased for the target population in our inferences. This is **coverage error**.\r\n\r\n<div style=\"text-align:center\">\r\n<img src=\"img/coverage_error.gif\" alt=\"Coverage error\">\r\n</div>\r\n\r\nOther variants of this error exist. With our USAID survey, by the time we showed up for an interview, a fraction of enterprises had gone out of business—and these should not have remained in the frame. New businesses were missing from the frame, because they did not yet exist in the months preceding our interviewing work.\r\n\r\n### A sample may be random, but not simple\r\n\r\nFinally, let's turn to **misspecification error**. This is error that occurs when analyzing data as if it were a simple random sample when in fact it isn't. If you downloaded the USAID dataset and performed a run-of-the-mill statistical _t_-test, you would commit this error. This is because survey data often involves sampling complexities that must be included in the analysis. \r\n\r\nOne such complexity is **clustering.** In sampling theory, clustering is a procedure where you sample naturally occurring groups of sampling units instead of sampling those units themselves. For the USAID survey, we used neighborhoods as clusters, and randomly chose neighborhoods around the kingdom to visit. Clustering typically increases error, so practitioners use it for convenience or to reduce cost. With the USAID survey, it allowed the team to interview multiple businesses in close physical proximity when going out for a field visit.\r\n\r\n<div style=\"text-align:center\">\r\n<img src=\"img/clusters.png\" alt=\"Clustering\"  width=\"600\">\r\n</div>\r\n    \r\nWe call the impact of the sampling scheme the **design effect** (_deff_). This measure expresses the loss of statistical efficiency from a complex data-gathering scheme compared to a simple random sample of the same size. In the USAID survey, we achieved a [median deff of 2.6](https://figshare.com/articles/online_resource/MSE_Survey_Weighting_Report_2018_/9848156). That means that confidence intervals would typically be sqrt(deff)= sqrt(2.6)=1.6 times larger under a correct analysis compared to a naïve analysis that used the sample standard deviation. That's a sizable difference!\r\n\r\n\r\n## Conclusion: know thy errors\r\n\r\nIf you go looking for them, there are plenty of statistical packages that address various types of error. In R, for example, the [`mice`](https://datascienceplus.com/imputing-missing-data-with-r-mice-package/) package nicely handles non-response and the [`survey`](https://r-survey.r-forge.r-project.org/survey/) package helps clustered samples. SPSS and SAS have similar routines/menus for complex surveys. Unfortunately, some types of error like undercoverage remain difficult to solve. \r\n\r\nWhether we adjust our analyses or not, an awareness of these errors can go a long way to make research better. Given the so-called [crisis of replication in science](https://www.vox.com/future-perfect/21504366/science-replication-crisis-peer-review-statistics), we need methods that guard against overly optimistic results. My hope is that as we start to discuss these errors more, that we can all start to make science more transparent and honest.\r\n\r\n\r\n##### References\r\n\r\n- Hoekstra, R, Morey R. D, Rouder, J. N, and Wagenmakers, E.-J. (2014) [\"Robust misinterpretation of confidence intervals,\"](http://www.ejwagenmakers.com/inpress/HoekstraEtAlPBR.pdf) Psychon Bull Rev. DOI 10.3758/s13423-013-0572-3.\r\n\r\n- USAID Jordan Local Enterprise Support Project (2015). “Survey of Jordanian Micro- and Small-Enterprises.” version 2.3.1. http://www.jordanlens.org\r\n\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}