{"name":"542 blog","tagline":"Musings about statistics and data science","body":"# A whole new world of errors\r\n\r\nHoekstra et al. have written about how university students do not have a precise understanding of confidence intervals (CIs), even when they have taken research methods courses. It turns out that that both trained and untrained minds look at the such classic intervals and want to believe—wrongly—that there is a 95% probability that the true parameter is in the interval [a,b]. \r\n\r\nAs an undergrad student of Political Science, I fell into this trap. But even after teaching myself to interpret confidence intervals correctly, it was not until I started working with complex surveys regularly that I realized how statistical software spit out confidence intervals that can often mislead readers.\r\n\r\nThe mistake is believing that these intervals capture all of your error. In fact, they only capture sampling error under idealized conditions. Sadly, reality is far from ideal, and non-sampling errors abound.\r\n\r\nThree types of non-sampling errors are particularly prevalent in the social sciences: non-response, under-coverage, and mis-specification error. Let’s jump on a magic carpet ride to explore these in turn.\r\n\r\n### Coming up short of your sampling target\r\n\r\nLet me illustrate non-response error with an enterprise survey I led while working with the USAID Jordan Local Enterprise Support Project. In this study, we conducted a survey with businesses that would inform ideas for new economic interventions. The survey needed to be large enough that we could drill-down into marginal groups and discover new insights.\r\n\r\nAlthough we planned for a sample of 6,385 businesses, by the time we wrapped our fieldwork, we only managed 4,721 interviews. This is because some owners refused to take part, while other business could not be located or were shut at the time of visit. We believe these problems were prevalent for non-registered businesses who feared being reported to authorities, and Bedouin tradespeople in the South. \r\n\r\nAs far as participation rates go for surveys, 74% is very desirable. Still, analyzing the USAID data without adjustment could lead to results that severely misstate the error. Why? Because statistical software assumes that data are “independent and identically distributed”—i.i.d. for short—but the non-response patterns mean that this condition no longer holds. Even for large datasets such as the USAID survey, nonresponse can be material enough to substantially bias your CIs.\r\n\r\n### The missing Uber driver\r\n\r\nWith the USAID survey, we wanted to build a master list of businesses that we could subsequently sample for an interview. Because we needed this list to capture the informal economy and gig-economy, our team had to build this sampling frame from scratch. We did this by going door-to-door in residential and commercial neighbourhoods (sampled at random), and asking if a business existed at those addresses. \r\n\r\nWhile this mechanism was pretty good, it undoubtedly missed certain businesses — businesses like Uber drivers, who might be out on the roads at the time we visited, or gig-workers with day jobs in larger companies. Samples drawn from this imperfect frame would not include those self-employed businesses and would therefore be biased for the target population make inferences for. This is coverage error.\r\n\r\nOther variants of this error exist. With our USAID survey, by the time we showed up for an interview, a fraction of enterprises had gone out of business—and these should not have remained in the frame. New businesses were missing from the frame, because they did not yet exist in the months preceding our interviewing work.\r\n\r\n### A sample may be random, but not simple\r\n\r\nFinally, let’s turn to mis-specification error. This is error that occurs when analyzing data as if it were a simple random sample when in fact it isn’t. If you downloaded the USAID dataset and performed a run-of-the-mill statistical t-test, you would commit this error. This is because survey data often involves sampling complexities that must be included in the analysis. \r\n\r\nOne such complexity is clustering. In sampling theory, clustering is a procedure where you sample naturally occurring groups of sampling units instead of sampling those units themselves. For the USAID survey, we used neighbourhoods as clusters, and randomly chose neighbourhoods around the kingdom to visit. Clustering typically increases error, so practitioners use it for convenience or to reduce cost. With the USAID survey, it allowed the team to interview multiple businesses in close physical proximity when going out for a field visit, reducing cost.\r\n\r\n\r\nWe call the impact of the sampling scheme the design effect (deff). This measure expresses the loss of statistical efficiency from a complex data-gathering scheme compared to a simple random sample of the same size. In the USAID survey, we achieved a median deff of 2.6. That means that confidence intervals would typically be √deff= √2.6=1.6 times larger under a correct analysis compared to a naïve analysis that used the sample standard deviation. That’s a sizeable difference!\r\n\r\n\r\n## CONCLUSION\r\n\r\nYet to be written.\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}