<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>542 blog by rtaph</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>542 blog</h1>
        <p>Musings about statistics and data science</p>

        <p class="view"><a href="https://github.ubc.ca/rtaph/542_blog">View the Project on GitHub <small>rtaph/542_blog</small></a></p>


      </header>
      <section>
        <h1>
<a id="a-whole-new-world-of-errors" class="anchor" href="#a-whole-new-world-of-errors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>A whole new world of errors</h1>
<p>Hoekstra et al. have <a href="http://www.ejwagenmakers.com/inpress/HoekstraEtAlPBR.pdf">written</a> about how university students do not have a precise understanding of confidence intervals (CIs), even when they have taken research methods courses. It turns out that that both trained and untrained minds look at the such classic intervals and want to believe—wrongly—that there is a 95% probability that the true parameter is in the interval [a,b].</p>
<p>As an undergrad student of Political Science, I fell into this trap. But even after teaching myself to interpret confidence intervals correctly, it was not until I started working with complex surveys regularly that I realized how statistical software spit out confidence intervals that can often mislead researchers.</p>
<p><ins>The mistake is believing that these intervals capture all of your error.</ins> In fact, they only capture sampling error under idealized conditions. Sadly, reality is far from ideal, and non-sampling errors abound.</p>
<p>Three types of non-sampling errors are particularly prevalent in the social sciences: non-response, under-coverage, and misspecification error. Let's jump on a magic carpet ride to explore these in turn.</p>
<h3>
<a id="coming-up-short-of-your-sampling-target" class="anchor" href="#coming-up-short-of-your-sampling-target" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Coming up short of your sampling target</h3>
<p>Let me illustrate <strong>non-response error</strong> with an <a href="https://jordanlens.org/research/">enterprise survey</a> I led while working with the USAID Jordan Local Enterprise Support Project. In this study, we conducted a survey with businesses that would inform ideas for new economic interventions. The survey needed to be large enough that we could drill-down into marginal groups and discover new insights.</p>
<div>
<img src="img/sample_losses.gif" alt="Sample losses">
</div>
<p>Although we planned for a sample of 6,385 businesses, by the time we wrapped our fieldwork, we only managed to create a respondent set of 4,721 interviews. This is because some owners refused to take part, while other business could not be located or were shut at the time of visit. We believe these problems were prevalent for non-registered businesses who feared being reported to authorities, and Bedouin tradespeople in the South.</p>
<p>As far as <a href="https://www.pewresearch.org/methods/2017/05/15/what-low-response-rates-mean-for-telephone-surveys/">participation rates go</a> for surveys, 74% is very desirable. Still, analyzing the USAID data without adjustment could lead to results that severely misstate the error. Why? Because statistical software assumes that data are “independent and identically distributed”—<em>i.i.d.</em> for short—but the non-response patterns mean that this condition no longer holds. Even for large datasets such as the USAID survey, nonresponse can be material enough to substantially understate the real width of your CIs.</p>
<h3>
<a id="the-missing-uber-driver" class="anchor" href="#the-missing-uber-driver" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The missing Uber driver</h3>
<p>With the USAID survey, we wanted to build a master list of businesses that we could subsequently sample for an interview. Because we needed this list to capture the informal economy and gig-economy, our team had to build this sampling frame from scratch. We did this by going door-to-door in residential and commercial neighborhoods (sampled at random), and asking if a business existed at those addresses.</p>
<p>While this mechanism was pretty good, it undoubtedly missed certain businesses — businesses like Uber drivers, who might be out on the roads at the time we visited, or gig-workers with day jobs in larger companies. Samples drawn from this imperfect frame would not include those self-employed businesses and would therefore be biased for the target population in our inferences. This is <strong>coverage error</strong>.</p>
<div>
<img src="img/coverage_error.gif" alt="Coverage error">
</div>
<p>Other variants of this error exist. With our USAID survey, by the time we showed up for an interview, a fraction of enterprises had gone out of business—and these should not have remained in the frame. New businesses were missing from the frame, because they did not yet exist in the months preceding our interviewing work.</p>
<h3>
<a id="a-sample-may-be-random-but-not-simple" class="anchor" href="#a-sample-may-be-random-but-not-simple" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>A sample may be random, but not simple</h3>
<p>Finally, let's turn to <strong>misspecification error</strong>. This is error that occurs when analyzing data as if it were a simple random sample when in fact it isn't. If you downloaded the USAID dataset and performed a run-of-the-mill statistical <em>t</em>-test, you would commit this error. This is because survey data often involves sampling complexities that must be included in the analysis.</p>
<p>One such complexity is <strong>clustering.</strong> In sampling theory, clustering is a procedure where you sample naturally occurring groups of sampling units instead of sampling those units themselves. For the USAID survey, we used neighborhoods as clusters, and randomly chose neighborhoods around the kingdom to visit. Clustering typically increases error, so practitioners use it for convenience or to reduce cost. With the USAID survey, it allowed the team to interview multiple businesses in close physical proximity when going out for a field visit.</p>
<div>
<img src="img/clusters.png" alt="Clustering" width="600">
</div>
<p>We call the impact of the sampling scheme the <strong>design effect</strong> (<em>deff</em>). This measure expresses the loss of statistical efficiency from a complex data-gathering scheme compared to a simple random sample of the same size. In the USAID survey, we achieved a <a href="https://figshare.com/articles/online_resource/MSE_Survey_Weighting_Report_2018_/9848156">median deff of 2.6</a>. That means that confidence intervals would typically be sqrt(deff)= sqrt(2.6)=1.6 times larger under a correct analysis compared to a naïve analysis that used the sample standard deviation. That's a sizable difference!</p>
<h2>
<a id="conclusion-know-thy-errors" class="anchor" href="#conclusion-know-thy-errors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conclusion: know thy errors</h2>
<p>If you go looking for them, there are plenty of statistical packages that address various types of error. In R, for example, the <a href="https://datascienceplus.com/imputing-missing-data-with-r-mice-package/"><code>mice</code></a> package nicely handles non-response and the <a href="https://r-survey.r-forge.r-project.org/survey/"><code>survey</code></a> package helps clustered samples. SPSS and SAS have similar routines/menus for complex surveys. Unfortunately, some types of error like undercoverage remain difficult to solve.</p>
<p>Whether we adjust our analyses or not, an awareness of these errors can go a long way to make research better. Given the so-called <a href="https://www.vox.com/future-perfect/21504366/science-replication-crisis-peer-review-statistics">crisis of replication in science</a>, we need methods that guard against overly optimistic results. My hope is that as we start to discuss these errors more, that we can all start to make science more transparent and honest.</p>
<h5>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h5>
<ul>
<li>
<p>Hoekstra, R, Morey R. D, Rouder, J. N, and Wagenmakers, E.-J. (2014) <a href="http://www.ejwagenmakers.com/inpress/HoekstraEtAlPBR.pdf">"Robust misinterpretation of confidence intervals,"</a> Psychon Bull Rev. DOI 10.3758/s13423-013-0572-3.</p>
</li>
<li>
<p>USAID Jordan Local Enterprise Support Project (2015). “Survey of Jordanian Micro- and Small-Enterprises.” version 2.3.1. <a href="http://www.jordanlens.org">http://www.jordanlens.org</a></p>
</li>
</ul>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.ubc.ca/rtaph">rtaph</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
